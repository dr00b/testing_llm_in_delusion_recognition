{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff44ee4",
   "metadata": {},
   "source": [
    "Installation\n",
    "```\n",
    "pip install spacy_experimental\n",
    "pip install chardet\n",
    "pip install thinc[torch]\n",
    "pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.1/en_coreference_web_trf-3.4.0a2-py3-none-any.whl\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd900881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_coref = spacy.load(\"en_coreference_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb10596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coref_clusters_1': [My dad, he, he, he, my dad's, his, my dad, he, his, he, his, his, him], 'coref_clusters_2': [the behavioral variant FTD, it, this disease], 'coref_clusters_3': [passed, that, diagnosis to death, his passing], 'coref_clusters_4': [My, my, my, I, my, I, my, I, I, I, I], 'coref_clusters_5': [wrote, it], 'coref_clusters_6': [one blessing, it], 'coref_clusters_7': [my mother, my mom, we], 'coref_clusters_8': [lost, that]}\n",
      "<class 'spacy.tokens.span_group.SpanGroup'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My dad',\n",
       " 'he',\n",
       " 'he',\n",
       " 'he',\n",
       " \"my dad's\",\n",
       " 'his',\n",
       " 'my dad',\n",
       " 'he',\n",
       " 'his',\n",
       " 'he',\n",
       " 'his',\n",
       " 'his',\n",
       " 'him']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp_coref(\"My dad passed away last summer after suffering from the behavioral variant FTD, he was older when he was diagnosed but he probably had it longer but we probably missed a lot of signs over the years. When the symptoms really started getting bad that's when my dad's doctor ordered an MRI and we learned that the frontal part of his brain was shrinking and in atrophy.  For my dad, from diagnosis to death it took less than two years, a year and nine and a half months to be exact.    As another poster mentioned I wrote a journal of everything he went through and it was challenging for sure as my mother and I were his caregivers the entire time.  If there was one blessing it was that he never lost his memory of who my mom and I were so that was a good thing.  I could write a book here on what we went through with this disease but almost seven months since his passing, I wish I could take care of him for just one more day.\")\n",
    "print(doc.spans)\n",
    "print(type(doc.spans.get(\"coref_clusters_1\")))\n",
    "s = doc.spans.get(\"coref_clusters_1\")\n",
    "[str(a) for a in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4da7fbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f26cf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coref_clusters_1': [mom, She, she, her, her, her, she, her, herself, she, her, her], 'coref_clusters_2': [the remote, it], 'coref_clusters_3': [them, them], 'coref_clusters_4': [Dr. PERSON, I, I, I, your]}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(\"Dr. PERSON, mom is behaving the opposite way. She lives in an assisted living facility and she is pushing the call button every few minutes to have them hand her the remote when it is sitting right next to her, wanting them to wipe her when she goes to the bathroom and many other things like that. The caregivers are so frustrated and the nurse is trying to get her to do these things for herself while she still can. I am frustrated and can’t be around her because I am just so exhausted and I feel like her slave. What is your advice?\")\n",
    "print(doc.spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8be0e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coref_clusters_1': [I, my, me, me, I, I, I, my, my, my, I, I, I], 'coref_clusters_2': [her, my mother, She, She, She, her, She, her, her, my mother, she, she], 'coref_clusters_3': [am, it], 'coref_clusters_4': [They, they], 'coref_clusters_5': [frying, it], 'coref_clusters_6': [am, it]}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(\"\"\"Yes but it all depends on the state of dementia of your LO. Anything I try to say, request her to do or discuss with my mother is met with hostility. She is deeply paranoid and suspicious. She makes unfounded and quite horrid accusations to and about me. She argues with me even when I am being nice to her. She argues about her arguing!!! Bascially I am her punch bag and it is soul destroying. If you haven't already get a PoA and do what you have to do without them making it harder for you. If day to day tasks become impossible to complete then get home help for assistance or check your LO into a care facility. There comes a point when the stress and aggravation is just not worth it. They have dementia what do they know? Many times I feel like my mother is frying my brain and it literally hurts my head and I just want out. Can't do it any more, but I am trapped and she knows it because as well as having dementia she is a manipulative selfish narcissist which is a terrible combination. Get help is all I can say. Trying to deal with contentious issues on a one to one basis with a LO who is resistant non compliant is, 99% of the time, going to fail.\"\"\")\n",
    "print(doc.spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ac3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coref_clusters_1': [my husband's dementia, it, it, it], 'coref_clusters_2': [my husband's, he, him, his, he, he, him, him, his, his, he], 'coref_clusters_3': [We, we, our, We, we], 'coref_clusters_4': [my, I, I, I, I]}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_coref(\"\"\"We embraced my husband's dementia because.... it is what it is.   When he first got the diagnosis we were mostly relieved, better than the dramas and psychosis that plagued him for a few years.   That had been a miserable time and I was his target.   Knowing what it was,  made our lives better.\n",
    "\n",
    "We ended up with a good medical team, the right meds after a few months of trials, government pension,  did all the legal paperwork while he could still function and we told people what he had with no shame or hesitation.\n",
    "\n",
    "7 years down with him at a moderate to severe stage and him sitting most days with his own thoughts, I think maybe his life is not so bad, no decisions, no bills, no driving, no responsibility,  not answerable for anything...... perpetual holiday of the mind.  It's then I think he's the lucky one.   I just get the work.\n",
    "\"\"\"\n",
    ")\n",
    "print(doc.spans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ed2b0fc",
   "metadata": {},
   "source": [
    "### Proposed Methodology\n",
    "1. Sort most common head coref clusters of interest (start w/ my then a word...). Count them, calculate distribution of length of references.\n",
    "2. Create list of relevant coref cluster heads.\n",
    "3. Determine threshold for \"talking about self\". What is length of \"I\" (knowledgeable informant) vs. the patient?\n",
    "\n",
    "### Proposed NLP Pipeline\n",
    "1. Remove single sentence comments\n",
    "2. NER, replace names\n",
    "3. Remove thank you's\n",
    "4. Apply coreference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ee49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "load_dotenv()\n",
    "\n",
    "comments = pd.read_sql(\"SELECT ROWID, * FROM comments\", sqlite3.connect(os.path.join(\"..\", \"data\", os.environ[\"SQLITE_DB_NAME\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "760f446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232214, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25164382439106703"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(comments.shape)\n",
    "sys.getsizeof(comments) / 1024**3 # GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714c1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\spacy_experimental\\lib\\site-packages\\spacy\\util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
    "nlp_sm.add_pipe(\"merge_noun_chunks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c134f71e",
   "metadata": {},
   "source": [
    "### Initial Filtering Dataset\n",
    "Reduce dataset size before apply compute heavy coref."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3987b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove replies: 232214 -> 179816 (77.44%)\n",
      "remove replies by channel owner: 179816 -> 179816 (100.00%)\n",
      "remove comments with only one sentence: 179816 -> 87476 (48.65%)\n"
     ]
    }
   ],
   "source": [
    "# Reduce Before Applying Coref\n",
    "#### Filtering Criteria\n",
    "comments[\"sentence_count\"] = comments[\"comment_text\"].apply(lambda x: len(list(nlp_sm(x).sents)))\n",
    "filter_1 = (comments.is_reply == 0)\n",
    "# filter_2 = (comments.reply_by_channel_owner == 0) redudant\n",
    "filter_3 = (comments.sentence_count) > 1\n",
    "\n",
    "filters = [\n",
    "    ('remove replies', filter_1),\n",
    "    ('remove comments with only one sentence', filter_3)\n",
    "]\n",
    "\n",
    "filtered = comments.copy()\n",
    "for name, bool_srs in filters:\n",
    "    orig_rows = filtered.shape[0]\n",
    "    filtered = filtered.loc[bool_srs, :]\n",
    "    updated_rows = filtered.shape[0]\n",
    "    print(f\"{name}: {orig_rows} -> {updated_rows} ({updated_rows / orig_rows:.2%})\")\n",
    "\n",
    "filtered.to_pickle(os.path.join(\"..\", \"data\", \"filtered_comments.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f980e4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215671    I understand what you going through. keep the ...\n",
       "3450      Yes I have.  Just recently.  Good resource for...\n",
       "162735    Would love to hear her on a decent piano. That...\n",
       "10660     OMG all of them!!!! But then I looked at the p...\n",
       "160900    Is it out of tune?...I forgot how a perfectly ...\n",
       "61022     Doc… I love you man, but vegetable carbohydrat...\n",
       "87825     My grandma has dementia..her memory lasts like...\n",
       "114940    even though she’s and actress, i HATE that bit...\n",
       "10805     Yes, going to make a binder for myself and my ...\n",
       "103976    Why do some people scratch their heads when th...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.comment_text.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfb62c7c",
   "metadata": {},
   "source": [
    "### Apply Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b3bca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 to 150, 0.00%\n"
     ]
    }
   ],
   "source": [
    "filtered = pd.read_pickle(os.path.join(\"..\", \"data\", \"filtered_comments.pkl\"))\n",
    "database_path = os.path.join(\"..\", \"data\", os.environ[\"SQLITE_DB_NAME\"])\n",
    "conn = sqlite3.connect(database_path)\n",
    "\n",
    "# iterate over filtered in batches\n",
    "batch_size = 1500\n",
    "for i in range(0, filtered.shape[0], batch_size):\n",
    "    print(f\"Processing {i} to {i+batch_size}, {i / filtered.shape[0]:.2%}\")\n",
    "    batch = filtered.iloc[i:i+batch_size, :].copy()\n",
    "    batch[\"coref_result\"] = batch[\"comment_text\"].apply(lambda x: nlp_coref(x))\n",
    "    batch[\"coref_result_json\"] = batch[\"coref_result\"].apply(lambda x: str(x.to_json()))\n",
    "    batch[\"coref_spans_json\"] = batch[\"coref_result\"].apply(lambda x: str(x.spans))\n",
    "    batch[\"coref_doc_bytes\"] = batch[\"coref_result\"].apply(lambda x: x.to_bytes())\n",
    "    batch.rename(columns={\"rowid\": \"comment_rowid\"}, inplace=True)\n",
    "    batch.drop(columns=[c for c in batch.columns if c not in [\"comment_rowid\", \"coref_result_json\", \"coref_spans_json\", \"coref_doc_bytes\"]], inplace=True)\n",
    "    batch.to_sql(con=conn, name=\"comments_coref\", if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c84f7b53",
   "metadata": {},
   "source": [
    "### Load Coref Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5c1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = os.path.join(\"..\", \"data\", os.environ[\"SQLITE_DB_NAME\"])\n",
    "conn = sqlite3.connect(database_path)\n",
    "\n",
    "comments_coref = pd.read_sql(\"SELECT * FROM comments_coref3\", conn)\n",
    "comments_coref[\"coref_result\"] = comments_coref[\"coref_doc_bytes\"].apply(lambda x: spacy.tokens.doc.Doc(nlp_coref.vocab).from_bytes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b8513e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unpack_coref_to_tokens(row):\n",
    "    lst = []\n",
    "    all_coref_chains = row[\"coref_result\"].spans\n",
    "    for span in all_coref_chains:\n",
    "        span_index = span.split(\"_\")[-1]\n",
    "        for j, ent in enumerate(all_coref_chains[span]):\n",
    "            p = nlp_sm(ent.text)\n",
    "            s = next(p.sents)\n",
    "\n",
    "            possessive_tup = None\n",
    "            root_tup = None\n",
    "            root_first = True\n",
    "            compressed_possessive = None\n",
    "            compressed_possessive_lemma = None\n",
    "\n",
    "            for i, t in enumerate(s):\n",
    "                if t.dep_ == \"poss\" and t.pos_ == \"PRON\":\n",
    "                    possessive_tup = (t.text, i)\n",
    "                    possessive_lemma = t.lemma_\n",
    "                elif t.dep_ == \"ROOT\":\n",
    "                    root_tup = (t.text, i, t.pos_)\n",
    "                    root_lemma = t.lemma_\n",
    "            if root_tup is not None and possessive_tup is not None:\n",
    "                root_first = root_tup[1] < possessive_tup[1]\n",
    "                if root_first:\n",
    "                    compressed_possessive = root_tup[0] + \" \" + possessive_tup[0] \n",
    "                    compressed_possessive_lemma = root_lemma + \" \" + possessive_lemma\n",
    "                else:\n",
    "                    compressed_possessive = possessive_tup[0] + \" \" + root_tup[0]\n",
    "                    compressed_possessive_lemma = possessive_lemma + \" \" + root_lemma\n",
    "                    \n",
    "                \n",
    "            row_dict = {\n",
    "                \"comment_rowid\": row[\"comment_rowid\"],\n",
    "                \"ref_chain_index\": span_index,\n",
    "                \"ref_index\": j,\n",
    "                \"original_token\": ent.text,\n",
    "                \"lower_token\": ent.text.lower(),\n",
    "                \"root\": s.root,\n",
    "                \"root_lemmatized\": root_lemma,\n",
    "                \"root_pos\": root_tup[2],\n",
    "                \"compressed_possessive\": compressed_possessive,\n",
    "                \"compressed_possessive_lemmatized\": compressed_possessive_lemma\n",
    "            }\n",
    "            lst.append(row_dict)\n",
    "    return lst\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d53a07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in comments_coref.iterrows():\n",
    "    lst = unpack_coref_to_tokens(row)\n",
    "    if i == 0:\n",
    "        df = pd.DataFrame(lst)\n",
    "    else:\n",
    "        df = pd.concat([df, pd.DataFrame(lst)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90de594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final_token\"] = df[\"compressed_possessive_lemmatized\"].fillna(df[\"root_lemmatized\"]).str.lower()\n",
    "df[\"final_token_len\"] = df[\"final_token\"].str.len()\n",
    "df[\"within_chain_max_len\"] = df.groupby([\"comment_rowid\", \"ref_chain_index\"])[\"final_token_len\"].transform(lambda x: x.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11730da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother ROOT NOUN 2 0\n"
     ]
    }
   ],
   "source": [
    "a = nlp_sm(\"mother\")\n",
    "for i, t in enumerate(a):\n",
    "    print(t.text, t.dep_, t.pos_, t.ent_iob, t.ent_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b01b06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_rowid</th>\n",
       "      <th>ref_chain_index</th>\n",
       "      <th>ref_index</th>\n",
       "      <th>original_token</th>\n",
       "      <th>lower_token</th>\n",
       "      <th>root</th>\n",
       "      <th>root_lemmatized</th>\n",
       "      <th>root_pos</th>\n",
       "      <th>compressed_possessive</th>\n",
       "      <th>compressed_possessive_lemmatized</th>\n",
       "      <th>final_token</th>\n",
       "      <th>final_token_len</th>\n",
       "      <th>within_chain_max_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>he</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>an attorney</td>\n",
       "      <td>an attorney</td>\n",
       "      <td>attorney</td>\n",
       "      <td>attorney</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>attorney</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>that</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>you</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>you</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>your</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>FTD</td>\n",
       "      <td>ftd</td>\n",
       "      <td>FTD</td>\n",
       "      <td>FTD</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ftd</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>this disease</td>\n",
       "      <td>this disease</td>\n",
       "      <td>disease</td>\n",
       "      <td>disease</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>disease</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>my</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>my</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>my</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>my mother</td>\n",
       "      <td>my mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>my mother</td>\n",
       "      <td>my mother</td>\n",
       "      <td>my mother</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>her</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>her</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>she</td>\n",
       "      <td>she</td>\n",
       "      <td>she</td>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>she</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>She</td>\n",
       "      <td>she</td>\n",
       "      <td>She</td>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>she</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>her</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>her</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>My sister and I</td>\n",
       "      <td>my sister and i</td>\n",
       "      <td>sister</td>\n",
       "      <td>sister</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>My sister</td>\n",
       "      <td>my sister</td>\n",
       "      <td>my sister</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>We</td>\n",
       "      <td>we</td>\n",
       "      <td>We</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>we</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    comment_rowid ref_chain_index  ref_index   original_token  \\\n",
       "25              1               2         10               he   \n",
       "26              1               3          0               we   \n",
       "27              1               3          1               we   \n",
       "28              1               4          0      an attorney   \n",
       "29              1               4          1             that   \n",
       "30              1               5          0              you   \n",
       "31              1               5          1              you   \n",
       "32              1               5          2             your   \n",
       "33              1               6          0              FTD   \n",
       "34              1               6          1     this disease   \n",
       "0               2               1          0               My   \n",
       "1               2               1          1                I   \n",
       "2               2               1          2               my   \n",
       "3               2               1          3                I   \n",
       "4               2               1          4                I   \n",
       "5               2               2          0        my mother   \n",
       "6               2               2          1              her   \n",
       "7               2               2          2              her   \n",
       "8               2               2          3              she   \n",
       "9               2               2          4              She   \n",
       "10              2               2          5              her   \n",
       "11              2               3          0  My sister and I   \n",
       "12              2               3          1               We   \n",
       "13              2               3          2               we   \n",
       "14              2               3          3               us   \n",
       "\n",
       "        lower_token      root root_lemmatized root_pos compressed_possessive  \\\n",
       "25               he        he              he     PRON                  None   \n",
       "26               we        we              we     PRON                  None   \n",
       "27               we        we              we     PRON                  None   \n",
       "28      an attorney  attorney        attorney     NOUN                  None   \n",
       "29             that      that            that     PRON                  None   \n",
       "30              you       you             you     PRON                  None   \n",
       "31              you       you             you     PRON                  None   \n",
       "32             your      your            your     PRON                  None   \n",
       "33              ftd       FTD             FTD    PROPN                  None   \n",
       "34     this disease   disease         disease     NOUN                  None   \n",
       "0                my        My              my     PRON                  None   \n",
       "1                 i         I               I     PRON                  None   \n",
       "2                my        my              my     PRON                  None   \n",
       "3                 i         I               I     PRON                  None   \n",
       "4                 i         I               I     PRON                  None   \n",
       "5         my mother    mother          mother     NOUN             my mother   \n",
       "6               her       her             her     PRON                  None   \n",
       "7               her       her             her     PRON                  None   \n",
       "8               she       she             she     PRON                  None   \n",
       "9               she       She             she     PRON                  None   \n",
       "10              her       her             her     PRON                  None   \n",
       "11  my sister and i    sister          sister     NOUN             My sister   \n",
       "12               we        We              we     PRON                  None   \n",
       "13               we        we              we     PRON                  None   \n",
       "14               us        us              we     PRON                  None   \n",
       "\n",
       "   compressed_possessive_lemmatized final_token  final_token_len  \\\n",
       "25                             None          he                2   \n",
       "26                             None          we                2   \n",
       "27                             None          we                2   \n",
       "28                             None    attorney                8   \n",
       "29                             None        that                4   \n",
       "30                             None         you                3   \n",
       "31                             None         you                3   \n",
       "32                             None        your                4   \n",
       "33                             None         ftd                3   \n",
       "34                             None     disease                7   \n",
       "0                              None          my                2   \n",
       "1                              None           i                1   \n",
       "2                              None          my                2   \n",
       "3                              None           i                1   \n",
       "4                              None           i                1   \n",
       "5                         my mother   my mother                9   \n",
       "6                              None         her                3   \n",
       "7                              None         her                3   \n",
       "8                              None         she                3   \n",
       "9                              None         she                3   \n",
       "10                             None         her                3   \n",
       "11                        my sister   my sister                9   \n",
       "12                             None          we                2   \n",
       "13                             None          we                2   \n",
       "14                             None          we                2   \n",
       "\n",
       "    within_chain_max_len  \n",
       "25                    10  \n",
       "26                     2  \n",
       "27                     2  \n",
       "28                     8  \n",
       "29                     8  \n",
       "30                     4  \n",
       "31                     4  \n",
       "32                     4  \n",
       "33                     7  \n",
       "34                     7  \n",
       "0                      2  \n",
       "1                      2  \n",
       "2                      2  \n",
       "3                      2  \n",
       "4                      2  \n",
       "5                      9  \n",
       "6                      9  \n",
       "7                      9  \n",
       "8                      9  \n",
       "9                      9  \n",
       "10                     9  \n",
       "11                     9  \n",
       "12                     9  \n",
       "13                     9  \n",
       "14                     9  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[25:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a50bf2c",
   "metadata": {},
   "source": [
    "### Compute Chain Level Features\n",
    "- Count of first-person subjective \"I\", filter out \"my\"\n",
    "- Longest lemmatized possessive phrase designated as \"head\", root is PNOUN or NOUN\n",
    "\n",
    "### Compute Comment Level Features\n",
    "- Balance \"I\" against narrated dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_experimental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "92b7bdbe9c8b4b2b7b656b32167ffa41b338465919cff767a784d15e226e5886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
